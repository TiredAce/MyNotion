## 第2章 改善神经网络：超参数调试，正则化及其优化

> [2.1 深度学习的实践方面](#2.1)
>

<h3 id = “2.1”>
2.1 深度学习的实践方面
</h3>

#### 2.1.1 训练，验证，测试集

在训练神经网络时，需要做出很多决策。例如：神经网络的层数，每层含有多少个单元，学习率，采用那些激活函数。

创建高质量的训练数据集，验证集和测试机才有助于提高循环效率。

一个训练数据通常会将这些数据划分成几个部分，一部分作为*训练集*，一部分作为*简单交叉验证集*，也称作验证集，最后一部分作为*测试集*。

*训练集*：英爱训练模型内参数的数据集。

*验证集*：用于训练过程中检验模型的状态，收敛情况。常用于调整超参数。还可以用来监控是否发生过拟合，一般来说验证集表现稳定后，若继续训练，训练集表现还会继续上升，但是验证集会出现不升反降的情况。

*测试集*：测试集用来评价模型泛化能力，再确定超参数之后，最后使用一个从没有见过的数据集来判断这个模型是否效果是否可以。

接下来，开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，来确定最终的模型。然后就可以在测试集上进行评估，胃口无偏估计算法的运行结果。

数据规模较小时，适用传统的划分比例，数据集规模较大的，验证集和测试集所占比例很小。

#### 2.1.2 偏方，方差

高偏方表示在训练集中表现不佳。

高方差表示在训练集中表现出过拟合现象，在验证集中的结果欠佳。

最优误差也被称为贝叶斯误差，在假设人眼识别的错误率接近$0\%$时，那么最优误差就接近$0\%$。

#### 2.1.3 机器学习基础

对于不同的神经网络架构，需要找到一个更合适解决次问题的新的网络架构。采用规模更大的网络通常都会有所帮助，延长训练时间不一定有用，但也没坏处。训练算法时，会不断尝试这些方法，直到解决掉偏差问题，直到拟合数据为止，至少能够拟合训练集。

如果网络够大，通常可以很好的拟合训练集，这只少可以拟合或者过拟合训练集。如果方差高，最好的解决方法就是采用更多数据，如果你能做到，会有一定的帮助，但有时候，无法获取更多数据，我们也可以藏式通过正则化来减少过拟合。

#### 2.1.4 正则化（Regularization）

深度学习可能存在过拟合问题———高方差，有两种解决方法，一个是正则化，另一个是准备更多的数据。准备足够多的训练数据可能获取成本较高，但正则化通常有助于避免过拟合或减少你的网络误差。

以逻辑回归为例，在逻辑回归函数中加入正则化，自需要添加参数$\lambda$，也就是正则化参数。添加正则化就是给损失函数加上一些限制，通过这种规则去规范他们接下来的迭代循环。

添加了正则化参数的损失函数如下：

$$
J(w,b) = \frac{1}{m} \sum^{m}_{i=1}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\parallel w \parallel ^2_2
$$ 
