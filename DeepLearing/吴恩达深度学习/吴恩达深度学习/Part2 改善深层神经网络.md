## 第2章 改善神经网络：超参数调试，正则化及其优化

> [2.1 深度学习的实践方面](#2.1)

<h3 id = “2.1”>
2.1 深度学习的实践方面
</h3>

#### 2.1.1 训练，验证，测试集

在训练神经网络时，需要做出很多决策。例如：神经网络的层数，每层含有多少个单元，学习率，采用那些激活函数。

创建高质量的训练数据集，验证集和测试机才有助于提高循环效率。

一个训练数据通常会将这些数据划分成几个部分，一部分作为*训练集*，一部分作为*简单交叉验证集*，也称作验证集，最后一部分作为*测试集*。

*训练集*：英爱训练模型内参数的数据集。

*验证集*：用于训练过程中检验模型的状态，收敛情况。常用于调整超参数。还可以用来监控是否发生过拟合，一般来说验证集表现稳定后，若继续训练，训练集表现还会继续上升，但是验证集会出现不升反降的情况。

*测试集*：测试集用来评价模型泛化能力，再确定超参数之后，最后使用一个从没有见过的数据集来判断这个模型是否效果是否可以。

接下来，开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，来确定最终的模型。然后就可以在测试集上进行评估，胃口无偏估计算法的运行结果。

数据规模较小时，适用传统的划分比例，数据集规模较大的，验证集和测试集所占比例很小。

#### 2.1.2 偏方，方差

高偏方表示在训练集中表现不佳。

高方差表示在训练集中表现出过拟合现象，在验证集中的结果欠佳。

最优误差也被称为贝叶斯误差，在假设人眼识别的错误率接近$0\%$时，那么最优误差就接近$0\%$。

#### 2.1.3 机器学习基础

对于不同的神经网络架构，需要找到一个更合适解决次问题的新的网络架构。采用规模更大的网络通常都会有所帮助，延长训练时间不一定有用，但也没坏处。训练算法时，会不断尝试这些方法，直到解决掉偏差问题，直到拟合数据为止，至少能够拟合训练集。

如果网络够大，通常可以很好的拟合训练集，这只少可以拟合或者过拟合训练集。如果方差高，最好的解决方法就是采用更多数据，如果你能做到，会有一定的帮助，但有时候，无法获取更多数据，我们也可以藏式通过正则化来减少过拟合。

#### 2.1.4 正则化（Regularization）

深度学习可能存在过拟合问题———高方差，有两种解决方法，一个是正则化，另一个是准备更多的数据。准备足够多的训练数据可能获取成本较高，但正则化通常有助于避免过拟合或减少你的网络误差。

正则化是指为解决适定性问题或过拟合而加入额外信息的过程。

以逻辑回归为例，在逻辑回归函数中加入正则化，自需要添加参数$\lambda$，也就是正则化参数。添加正则化就是给损失函数加上一些限制，通过这种规则去规范他们接下来的迭代循环。

添加了正则化参数的损失函数如下：

$$
J(w,b) = \frac{1}{m} \sum^{m}_{i=1}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\parallel w^{[l]} \parallel ^2_2
$$

>**L1范数**
>
>`L1`范数是我们常见的一种范数，它的定义如下：
>$$
>\parallel x \parallel_1 = \sum^{n}_{i=1}|x_i|
>$$
>表示向量`x`中非零绝对值的和。
>
>`L1`范数有很多的名字，例如熟悉的哈密顿距离。由于`L1`范数的天然性质，对`L1`优化的解是一个稀疏解，因此`L1`范数也被叫做稀疏规则算子。通过`L1`可以实现特征的稀疏，去点一些没有信息的特征。
>
>**L2范数**
>
>L2范数是最常见最常见的范数，用来度量欧式距离就是用的`L2`范数，它的定义如下：
>$$
>\parallel x \parallel_2 = \sqrt{\sum^{n}_{i = 1}x_i^2}
>$$
>`L2`范数通常会被用来做优化函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。

这里的正则化省略了`b`，那是因为`w`可能包含了很多参数，我们不可能你和所有参数，而`b`只是单个数字，所以`w`几乎涵盖了所有参数，加上了`b`，其实没有多大的影响，但是通常不略不计。直观来将`b`是一个偏置参数，只会影响拟合结果的偏移程度，但不会影响拟合的形状，因此不需要对其进行正则化。

如果用的是`L1`正则化，`w`最终会是稀疏的，也就是说`w`向量中会有很多`0`。但是，实际上虽然`L1`正则化能使模型变得稀疏，却没有降低太多存储内存，所有并不是`L1`正则化的目的，至少不是为了压缩模型，在训练神经网络时，越来越倾向使用`L2`正则化。

正则化实际上就是在最小化成本函数的同时，保证参数不会太大，参数太大，对模型越敏感，就会使得分类结果越扭曲。

将成本函数加上了正则化系数之后，计算出来的$dw^{[l]}$为$(from \ backprop) + \frac{\lambda}{m} w^{[l]}$，最后用该式子去迭代$w^{[l]}$。
$$
w^{[l]} := w^{[l]} - \alpha dw^{[l]} = w^{[l]} - \alpha [(from\ backpop) + \frac{\lambda}{m}w^{[l]}] = (1 - \frac{\alpha \lambda}{m})w^{[l]} - \alpha (from\ backpop)
$$
再上式子中$w$的权重为$(1 - \frac{\alpha \lambda}{m})$，因此，`L2`正则化也被称为“权重衰减”。

就是通过降低参数$w$的大小范围，从而降低模型复杂度，来解决过拟合问题。

**参考资料：**

- [如何解决过拟合问题？L1、L2正则化及Dropout正则化讲解](https://www.bilibili.com/video/BV18r4y1M71J/?vd_source=61c3f696848d48c33298883fd1df4ef0)

#### 2.1.5 为何正则化有利于预防过拟合？

直观的理解就是如果正则化$\lambda$设置得足够大，权重矩阵$W$被设置为接近于$0$的值，也就是把多隐藏单元的权重设置为$0$,于是基本上消除了这些隐藏单元的许多影响。这会大大简化了神经网络会变成一个很小的网络。

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230331171841188.png" alt="image-20230331171841188" width="400px" /></div>

这会使得模型小到如同一个逻辑回归单元，但是深度却很大，这样会使网络从过拟合状态进入到欠拟合状态，但是如果将$\lambda$的值调节适中，那么会有一个适中的模型结果。

更加直观的方式，为什么正则化可以预防过拟合，假设我们使用的双曲正切激活函数，那么只要$z$很小，那么我们就利用了双曲正切函数的线性状态。当$z$变大或者是变小，那么激活函数开始变得非线性。每层都是线性函数，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，最终也只能计算线性函数，因此它不适用于非常复杂的决策，以及过度拟合数据集的非线性决策边界。

#### 2.1.6 dropout正则化

假设一个神经网络结构存在过拟合，这就是`dropout`要处理的，我们复制这个神经网络，遍历网络的每层，并设置消除神经网络中节点的概率。假设网络中的每一层每个节点都已抛硬币的方式设置概率，每个节点得以保留或者消除的概率都是一半一半，设置完节点的概率，我们会消除一些节点，最后得到一个节点更少，规模更小的网络，然后`backprop`方法训练。

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230331201008894.png" alt="image-20230331201008894" width="400px"/></div>

`dropout`方法有很多中，接下来讲的是最常用的方法，即反向随机失活（inverted dropout），以下用一个三层的网络来举例说明。首先要定义向量$d$，$d^{[3]}$表示一个三层的`dropout`向量：

```python
d3 = np.random.randn(a3.shape[0], a3.shape[1]) < keep_prob
```

然后看它是否小于某个数，这个数叫做`keep_prob`，它表示保留某个隐藏单元的概率。假设我们让`keep_prob = 0.8`，那么就是让`d`中对应为`1`的概率为`0.8`，对应为`0`的概率为`0.2`。再与`a3`相乘就能得到对应位置归零。

```python
a3 = np.multiply(a3, d3)
```

最后，向外拓展$a^{[3]}$，除以`keep_prob`参数。

```python
a3 /= keep_prob
```

这里除去`keep_prob`参数是为了保证无偏估计，期望公式$E(x) = 0.2 \times0+0.8x = 0.8x$，因此只要再除以`keep_prob`就能保证期望不变了。

#### 2.1.7 理解dropout

直观上来理解：不要依赖任何一个特征，因为该单元的输入可能随时被消除，所以不会给任何一个怕输入加上太多的权重，因为它随时都可能被删除，因此该单元通过这种方式传播下去。`dropout`将产生收缩权重的平方范数效果，和之前讲的`L2`正则化类似。

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230331210614546.png" alt="image-20230331210614546" width="400px"/></div>

总之，如果你担心某些层更容易发生过拟合，可以把某些层的`keep_prob`值设置的比其他层更小一点，缺点就是为了使用交叉验证，你需要搜索更多的超级参数。另一种方案就是对某些层使用，某些层不去使用。

牢记一点就是，`dropout`是一种正则化方法，它有助于预防过拟合，因此除非算法出现过拟合现象，不然不用去使用`dropout`，所以它在其他领域用的比较少，主要存在于计算机视觉领域。

#### 2.1.8 其他正则化方法

1. 数据扩增

假设你正在拟合猫咪的数据，如果想通过扩增训练数据来解决过拟合，但扩增数据代价太高，但我们可以通过增加这类图片来增加训练集，例如水平翻转图片，并把它们加入训练集，这可以让训练集增大一倍。除了反转图片。还可以裁剪图片。

2. early stopping

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230331235020387.png" alt="image-20230331235020387" width="600px"/></div>

在训练过程时，希望训练误差，代价函数$J$都在下降，，可以发现，验证集误差通常会先呈下降趋势，然后在某个节点开始上升，`early stopping`的作用就是在神经网络在这个迭代过程中表现的很好了，那几直接停止训练，得到验证集误差。

#### 2.1.9 归一化输入

在神经网络训练中，其中一个加速训练的方法就是归一化输入，假设一个训练集有两个特征，输入特征为$2$维，归一化需要两个步骤：

1. 零均值

2. 归一化方差

第一步是零均值化，$\mu = \frac{1}{m} \sum^{m}_{i = 1}x^{(i)}$，它是一个向量，$x$等于每个训练数据$x$减去$\mu$，意思就是移动训练数据，直到它完成零均值。

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230401000254630.png" alt="image-20230401000254630" width="600px"/></div>

第二步是归一化方差，注意特征$x_1$的方差要比$x_2$的方差要大的多，我们要做的是给$\sigma$赋值，$\sigma^2 = \frac{1}{m} \sum^{m}_{i = 1}(x^{(i)})^2$，这是节点$y$的平方，样本的每一个特征都有方差，因为第一步中我们保证了样本的均值为$0$，因此样本的元素的平方就是方差，在把所有数据除以向量$\sigma$，最终变成下面图：

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230401001323504.png" alt="image-20230401001323504" width="600px"/></div>

要注意，$x_1$和$x_2$的方差都等于$1$，如果你用它来调整训练数据，那么用相同的$\mu$和$\sigma$来归一化测试集。所以你要用同样的方法调整测试集，而不是分别去预估。因为我们希望无论是训练数据还是测试数据，都是通过相同$\mu$和$\sigma ^2$定义的相同数据转换，其中的均值方差都是从训练集中计算得到的。

为什么需要归一化输入特征？非归一化的代价函数和归一化的代价函数分别如下所示：

<div align=center><img src="./../assets/blog_res/Part2 改善深层神经网络.assets/image-20230401002358527.png" alt="image-20230401002358527" width="600px"/></div>

如果你归一化特征，代价函数平均起来看更对称，如果你在上图这样的代价函数上运行梯度下降法。你必须使用一个非常小的学习率，因此如果是这个位置，梯度下降法可能需要多次迭代过程，直到找到最小值。如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都唔那个够更直接找到最小值，你可以子啊梯度下降法中使用更大步长，而不需要向左图那样反复执行。

如果特征值处于不同范围内，可能有些特征值从$0$到$1$，有些从$1$到$1000$，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。执行这类归一化并不会产生什么危害。
