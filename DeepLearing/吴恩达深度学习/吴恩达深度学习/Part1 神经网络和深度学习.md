## Part1 神经网络和深度学习（Neural Networks and Deep Learning）···

>[1.1 什么是神经网络](#1.1)
>
>[1.2 逻辑回归](#1.2)
>
>[1.3](#1.3)

<h3 id = "1.1">
1.1 什么是神经网络
</h3>


“深度学习”指的是训练神经网络。以下介绍直观的基础知识。

假设我们要根据房屋的面积，去预测价格，根据目前的数据，我们可以对这些点做线性回归，由于价格不可能为`0`，因此需要对直线做修正，这样的函数称为“修正线性单元”（ReLU）。

<div align=center>
<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230322223030296.png" alt="image-20230322223030296" style="zoom:67%;" width="600px" />
</div>

实际上的神经网络，就是在大量训练集下，通过计算从x到y的精准映射函数。

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230322223702026.png" alt="image-20230322223702026" style="zoom:67%;" width="600px"/>

<h3 id = "1.2">
1.2 逻辑回归
</h3>

逻辑回归是一个用于二分分类(Binary Classification)的算法。

例如给出一张图片，你要得到这张图片是否是一只猫。

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230322230834256.png" alt="image-20230322230834256" style="zoom:50%;" />

#### 1.2.1 训练样本

在计算机中，保存一张图片，要保存三个独立矩阵，分别对应红、绿、蓝三个颜色通道。

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230322232548581.png" alt="image-20230322232548581" style="zoom:50%;" />

假设计算机张每个独立矩阵是$64 \times 64$,现在将图片中所有的数据放入一个特征矩阵之中，那么这个特征矩阵就有$64 \times 64 \times 3=12288$个元素单元，因此输入的特征向量的维度为`12288`。

我们用一对`(x,y)`来表示一个单独的样本，其中$x\in R^{n_x},y\in  \{ 0,1  \} $。

那么`m`个训练样本就是:$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$

我们通常用小写`m`来表示训练样本个数，有时候为了强调训练样本的数量，可以写作$m=m_{train}$；我们用$m_{test}$来表示测试集的样本数。

最后，用更紧密的符号表示训练集，定义一个矩阵$X$,将训练集分别放到每一列中，要主要这个训练矩阵式行向量堆叠还是列向量堆叠。同理建立结果输出矩阵$Y$。

#### 1.2.1 映射函数

对于给出的$x$，我们想要预测$\widehat{y}=P(y=1|x)$能告诉你这张图片是猫的概率，那么已知逻辑回归的参数是$w$也是一个$n_x$维度的向量，而$b$是一个常数。然后得到输出函数:

$$\hat{y} = w^{T}x+b$$

因为最终要将概率定到0~1，所以需要一个映射函数将概率作用到这个量上，那就是`sigmoid`函数。

$$\hat{y} = \sigma(w^Tx+b)$$

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323160133930.png" alt="image-20230323160133930" width="400px"  />

$\sigma(z)=\frac{1}{1 + e^{-z}}$

当参数$z$趋向正无穷时，函数值趋向于$1$。当参数$z$趋向负无穷时，函数值趋向于$0$。

因此在做逻辑回归的时候，实际上就是去学习训练参数$w，b$。直到将$\hat{y}$训练成一个较好的估计。

我们通常会把参数$w$和参数$b$分开，这里的$b$对应一个拦截器。

#### 1.2.3 损失函数

*损失函数*也叫做*误差函数*，可以用来衡量算法的运行情况。通俗的讲法就是通过$L$函数来衡量你的预测输出值$\hat{y}$和$y$的实际值有多接近。*损失函数衡量的是在单个训练样本上的表现。*

这里使用的损失函数如下所示：

$$L(\hat{y},y)=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$$

为了更好的来理解，为什么这个函数能够起到作用，我们举以下两个例子：

如果我们说$y=1$,那么损失函数就取到前一项，即$L(\hat{y},y)=-log\hat{y}$。效果越好也就是对应着让损失函数越小越好，这就表示让$log\hat{y}$越大越好，也就是让$\hat{y}$越大越好，由于有$\sigma$函数将$\hat{y}$限制在`0~1`之间，因此也就是让$\hat{y}$越接近`1`越好。

#### 1.2.4 成本函数

为了训练逻辑回归模型的参数$w$以及$b$，需要定义一个成本函数。*成本函数衡量的是全体训练样本上的表现*。它的定义式如下：

$$J(w,b)=\frac{1}{m}\sum^{m}_{i=1}L(\hat{y}^{(i)},y^{(i)})$$

因此，在训练逻辑回归模型时，需要让成本函数$J$尽可能地小。

#### 1.2.5 梯度下降法

我们将成本函数的对应的图像展示，如下图所示：

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323164703847.png" alt="image-20230323164703847" width="400px" />

该函数类型为凸函数，凸函数的只存在一个最优解，在其底部。凸函数的这一性质是我们使用逻辑回归的这一特定成本函数$J$的重要原因之一。

为了找到更好的参数值，我们要做的就是用某初始值，初始化$w$和$b$,对于逻辑回归而言，几乎是任意的初始化方法都有效。梯度下降法所做的就是从初始点开始朝最陡的下坡方向走一步。经过多次迭代之后，到达局部最优解或是全局最优解。

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323165433920.png" alt="image-20230323165433920" width="400px" />

为了更好的说明，我们来看一些函数。如下图所示：

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323165640131.png" alt="image-20230323165640131" width="300px"/>

我们将$b$维度舍去，留下了$w$维度，得到如上形式的成本函数，我们的目标是找到成本函数的最小值，通过梯度下降法，我们将重复进行如下操作：

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323165853174.png" alt="image-20230323165853174" width="400px" />

在上述公式中有两点需要注意，首先$\alpha$表示学习率，学习率可以控制每次迭代或者是梯度下降法中的步长，至于$\alpha$如何选择在后文叙述。其次这里的导数，这就是对参数$w$的更新或者变化量，当我们编写代码时，我们用`dw`来表示导数。因此我们用$w:=w-\alpha dw$来表示。同理也可以用这种方式表示$b$。

#### 1.2.6 计算图

假设我们有这样一个函数$J(a,b,c)=3(a+bc)$,那么我们就能画出这样的流程图。

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323172053093.png" alt="image-20230323172053093" width="500px" />

那么如果我们要计算成本函数$J$相对于每个目标函数的变化情况，我们就能使用*微积分中的链式法则*，方向的通过计算图来计算出变化率。

在编写代码时，我们规定对于最终变量$J$的导数，我们用`dvar`来表示$\frac{d_{FindOutputVar}}{d_{Var}}$。

#### 1.2.7 使用计算图实现梯度下降法

现在写出样本的偏导数流程图，假设样本有两个特征$x_1,x_2$,因此我们有参数$w_1,w_2,b$，用来计算`z`的偏导数计算公式:

$$z=w_1 x_1 + w_2x_2 + b$$

通过`sigma`函数将`z`函数映射成$\hat{y}$:

$$\hat{y}=a=\sigma(z)$$

最后得到损失函数$L$:

$$L(a,y)=-(yloga+(1-y)log(1-a))$$

<img src="./../assets/blog_res/Part1 神经网络和深度学习.assets/image-20230323175716700.png" alt="image-20230323175716700" width="600px" />

#### 1.2.8 向量化

假如有$m$个样本，每个样本有$n$个特征，那么如果要计算梯度下降法所需要的算法时间复杂度为$O(nm)$,但是在深度学习中，样本的数量会十分大，那么如果使用`for`循环就会让训练的时间大幅度提升，因此我们使用向量化的方法来加速计算。
