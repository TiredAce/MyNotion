## 第2章 神经网络基础

> [2.1 机器学习基本概念](#2.1)

<h3 id = "2.1">
2.1 机器学习基本概念
</h3>

#### 2.1.1 机器学习分类

机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。即让计算机从数据中去挖掘有价值的信息。

根据训练数据是否标签，机器学习可以分为监督学习、半监督学习和无监督学习。

从算法输出的形式分类，可以分为分类算法和回归算法，这两类问题都属于监督学习的范畴。

*分类问题*：指的是模型的输入值为离散值。

*回归问题*：指的是模型的输出值为连续值。

#### 2.1.2 机器学习流程概述

在机器学习中通常需要如下几个步骤：

1. 提取商品图片的特征。
2. 建立模型，在定义好特征后，需要选择一个合适的模型来建模。
3. 确定损失函数和进行优化求解。

#### 2.1.3 常见的损失函数

1. 平方损失函数

平方损失函数定义如下：

$$
L(y, \hat{y}) = \frac{1}{N} \sum^{N}_{i=1}(y-\hat{y})^2
$$

其中$N$是样本数量，它衡量的是模型预测的结果与标签之间的平方差。常用于回归类问题。

2. 交叉熵损失

交叉熵损失常用于分类问题中，分类模型通常输出类别的概率分布，交叉熵衡量的是数据标签的真是分布和分类模型预测的概率分布之间的差异程度，损失值越小，它们之间的差异就越小，模型就能准确的进行预测。离散形式如下：
$$
L(y, \hat{y}) = H(p, q)= - \frac{1}{N}\sum^{N}_{i = 1}p(y_i|x_i)log[q(\hat{y}_i|x_i)]
$$
其中$p,q$分别表示数据标签的真实分布和模型预测给出的分布，$p(y_i|x_i)$表示样本$x_i$标签的真实分布。可以看出这种情况下，最小化交叉熵损失的本质就是最大化样本标签的似然概率。

对于二分类来说，使用逻辑回归可以得到样本$x_i$属于类别`1`的概率$q(y_i=1|x_i)$，那么样本属于类别``0`的概率为$1-q(y_i = 1 | x_i)$，可以得到逻辑回归的损失函数，它也被称为二元交叉熵损失。
$$
L(y, \hat{y}) = -\frac{1}{N} \sum^{N}_{i = 1}[y_ilogq(y_i=1| x_i) + (1 - y_i)log(1-q(y_i = 1 | x_i))]
$$

#### 2.1.4 梯度下降算法

1. 梯度下降算法

机器学习中很多问题本质上都是求解优化相关的问题，找到合适的参数以期最小化损失函数值。求解类似的优化问题，有很多成熟的方法可以参考，梯度下降就是一种典型的方法。它利用梯度信息，通过不断迭代调整参数来寻找合适的解。

取$\Delta x=-\alpha f'(x)$，可以保证更新后$x$可以使得函数值减小，这里的$\alpha$是一个超参数，用于调整每次更新的步长，称为学习率。

机器学习中优化的目标是最小化损失函数，通过梯度下降的方法进行求解的过程分为以下几步，算法过程如下所示。首先，通过随机化参数；接下来，预测，计算损失值；然后使用损失值计算梯度；最后基于梯度调整参数，得到迭代之后的参数。重复上述过程，直到达到停止条件。

2. 随机梯度下降算法

梯度下降算法在数据规模
